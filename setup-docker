## Why Docker?
Docker is an open platform for developing, shipping, and running applications. 
Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. 
With Docker, you can manage your infrastructure in the same ways you manage your applications. 
By taking advantage of Docker’s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.

You can download and install Docker on multiple platforms.
 https://docs.docker.com/get-docker/
 
Check versions
Ensure your versions of docker and docker-compose are up-to-date and compatible with Docker.app. 
 $ docker --version
 
Open a command-line terminal and test that your installation works by running the simple Docker image, hello-world:
  $ docker run hello-world

...
  Unable to find image 'hello-world:latest' locally
  latest: Pulling from library/hello-world
  ca4f61b1923c: Pull complete
  Digest: sha256:ca0eeb6fb05351dfc8759c20733c91def84cb8007aa89a5bf606bc8b315b9fc7
  Status: Downloaded newer image for hello-world:latest

  Hello from Docker!
  This message shows that your installation appears to be working correctly.
...

Now that you’ve set up your development environment, thanks to Docker Desktop, you can begin to develop containerized applications. 
In general, the development workflow looks like this:

1. Create and test individual containers for each component of your application by first creating Docker images.
2. Assemble your containers and supporting infrastructure into a complete application.
3. Test, share, and deploy your complete containerized application.

Create the images that your containers will be based on. 
Remember, a Docker image captures the private filesystem that your containerized processes will run in; 

Dockerfiles describe how to assemble a private filesystem for a container, and can also contain some metadata describing how to run a container based on this image. 
The bulletin board app Dockerfile looks like this:

...
# Use the official image as a parent image
FROM node:current-slim

# Set the working directory
WORKDIR /usr/src/app

# Copy the file from your host to your current location
COPY package.json .

# Run the command inside your image filesystem
RUN npm install

# Inform Docker that the container is listening on the specified port at runtime.
EXPOSE 8080

# Run the specified command within the container.
CMD [ "npm", "start" ]

# Copy the rest of your app's source code from your host to your image filesystem.
COPY . .
...

Writing a Dockerfile is the first step to containerizing an application. You can think of these Dockerfile commands as a step-by-step recipe on how to build up your image. This one takes the following steps:

1. Start FROM the pre-existing node:current-slim image. 
This is an official image, built by the node.js vendors and validated by Docker to be a high-quality image containing the Node.js Long Term Support (LTS) interpreter and basic dependencies.
2. Use WORKDIR to specify that all subsequent actions should be taken from the directory /usr/src/app in your image filesystem (never the host’s filesystem).
3. COPY the file package.json from your host to the present location (.) in your image (so in this case, to /usr/src/app/package.json)
4. RUN the command npm install inside your image filesystem (which will read package.json to determine your app’s node dependencies, and install them)
5. COPY in the rest of your app’s source code from your host to your image filesystem.

You can see that these are much the same steps you might have taken to set up and install your app on your host. 
However, capturing these as a Dockerfile allows you to do the same thing inside a portable, isolated Docker image.
The steps above built up the filesystem of our image, but there are other lines in your Dockerfile.

The CMD directive is the first example of specifying some metadata in your image that describes how to run a container based on this image. 
In this case, it’s saying that the containerized process that this image is meant to support is npm start.
The EXPOSE 8080 informs Docker that the container is listening on port 8080 at runtime.

What you see above is a good way to organize a simple Dockerfile; 
1. always start with a FROM command, follow it with the steps to build up your private filesystem, and conclude with any metadata specifications.

If you don’t yet have a Docker ID, follow these steps to set one up; this will allow you to share images on Docker Hub.
1. Visit the Docker Hub sign up page, https://hub.docker.com/signup.
2. Fill out the form and submit to create your Docker ID.
3. Verify your email address to complete the registration process.
4. Click on the Docker icon in your toolbar or system tray, and click Sign in / Create Docker ID.
5 .Fill in your new Docker ID and password. After you have successfully authenticated, your Docker ID appears in the Docker Desktop menu in place of the ‘Sign in’ option you just used.

You can do the same thing from the command line by typing docker login.

At this point, you’ve set up your Docker Hub account and have connected it to your Docker Desktop. 
1. Click on the Docker icon in your menu bar, and navigate to Repositories > Create. 
2. You’ll be taken to a Docker Hub page to create a new repository.
3. Fill out the repository name as <NAME>. Leave all the other options alone for now, and click Create at the bottom.

Now you are ready to share your image on Docker Hub, but there’s one thing you must do first: 
images must be namespaced correctly to share on Docker Hub. 
Specifically, you must name images like <Docker ID>/<Repository Name>:<tag>.

You can relabel your <Name>:<tag> image:
  docker image tag <Name>:<tag> <Docker ID>/<Name>:<tag>

Finally, push your image to Docker Hub:
  docker image push <Docker ID>/<Name>:<tag>

Visit your repository in Docker Hub, and you’ll see your new image there. Remember, Docker Hub repositories are public by default.
